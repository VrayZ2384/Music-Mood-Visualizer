{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VrayZ2384/Music-Mood-Visualizer/blob/main/Music_MultiModal_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs_K7yH22P6q"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/openai/CLIP.git\n",
        "#https://github.com/openai/CLIP\n",
        "#CLIP (Contrastive Language-Image Pre-Training)\n",
        "#Learning Transferable Visual Models From Natural Language Supervision\n",
        "#Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\n",
        "#Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\n",
        "\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "#https://github.com/CompVis/taming-transformers\n",
        "#Taming Transformers for High-Resolution Image Synthesis\n",
        "#Patrick Esser, Robin Rombach, BjÃ¶rn Ommer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rSfqKwK38sA"
      },
      "outputs": [],
      "source": [
        "## install some extra libraries\n",
        "!pip install --no-deps ftfy regex tqdm\n",
        "!pip install omegaconf==2.0.0 pytorch-lightning==1.0.8\n",
        "!pip uninstall torchtext --yes\n",
        "!pip install einops\n",
        "!pip install librosa\n",
        "!pip install pytube\n",
        "!pip install requests\n",
        "!pip install flask\n",
        "!pip install flask-cors\n",
        "!pip install tqdm\n",
        "# Install ngrok\n",
        "!pip install pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eU0TkyTJ4P9R"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import torch, os, imageio, pdb, math\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "import librosa\n",
        "import pytube\n",
        "from pyngrok import ngrok\n",
        "import shutil\n",
        "from http.server import SimpleHTTPRequestHandler\n",
        "import http.server\n",
        "import socketserver\n",
        "import json\n",
        "import requests\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from IPython.display import Image, display\n",
        "from PIL import Image as PILImage\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import yaml\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from CLIP import clip\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tZIgwDiWvzl8"
      },
      "outputs": [],
      "source": [
        "## helper functions\n",
        "\n",
        "def show_from_tensor(tensor):\n",
        "  img = tensor.clone()\n",
        "  img = img.mul(255).byte()\n",
        "  img = img.cpu().numpy().transpose((1,2,0))\n",
        "\n",
        "  plt.figure(figsize=(10,7))\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "def norm_data(data):\n",
        "  return (data.clip(-1,1)+1)/2 ### range between 0 and 1 in the result\n",
        "\n",
        "### Parameters\n",
        "learning_rate = .5\n",
        "batch_size = 1\n",
        "wd = .1\n",
        "noise_factor = .22\n",
        "\n",
        "total_iter=400\n",
        "im_shape = [450, 450, 3] # height, width, channel\n",
        "size1, size2, channels = im_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgzYorUjv2A9"
      },
      "outputs": [],
      "source": [
        "### CLIP MODEL ###\n",
        "clipmodel, _ = clip.load('ViT-B/32', jit=False)\n",
        "clipmodel.eval()\n",
        "print(clip.available_models())\n",
        "\n",
        "print(\"Clip model visual input resolution: \", clipmodel.visual.input_resolution)\n",
        "\n",
        "device=torch.device(\"cuda:0\")\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lrDSmMQv3Vn"
      },
      "outputs": [],
      "source": [
        "## Taming transformer instantiation\n",
        "\n",
        "%cd taming-transformers/\n",
        "\n",
        "!mkdir -p models/vqgan_imagenet_f16_16384/checkpoints\n",
        "!mkdir -p models/vqgan_imagenet_f16_16384/configs\n",
        "\n",
        "if len(os.listdir('models/vqgan_imagenet_f16_16384/checkpoints/')) == 0:\n",
        "   !wget 'https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1' -O 'models/vqgan_imagenet_f16_16384/checkpoints/last.ckpt'\n",
        "   !wget 'https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1' -O 'models/vqgan_imagenet_f16_16384/configs/model.yaml'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNCsUNG5v79q"
      },
      "outputs": [],
      "source": [
        "from taming.models.vqgan import VQModel\n",
        "\n",
        "def load_config(config_path, display=False):\n",
        "   config_data = OmegaConf.load(config_path)\n",
        "   if display:\n",
        "     print(yaml.dump(OmegaConf.to_container(config_data)))\n",
        "   return config_data\n",
        "\n",
        "def load_vqgan(config, chk_path=None):\n",
        "  model = VQModel(**config.model.params)\n",
        "  if chk_path is not None:\n",
        "    state_dict = torch.load(chk_path, map_location=\"cpu\")[\"state_dict\"]\n",
        "    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "  return model.eval()\n",
        "\n",
        "def generator(x):\n",
        "  x = taming_model.post_quant_conv(x)\n",
        "  x = taming_model.decoder(x)\n",
        "  return x\n",
        "\n",
        "taming_config = load_config(\"./models/vqgan_imagenet_f16_16384/configs/model.yaml\", display=True)\n",
        "taming_model = load_vqgan(taming_config, chk_path=\"./models/vqgan_imagenet_f16_16384/checkpoints/last.ckpt\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jE4bigWQwNae"
      },
      "outputs": [],
      "source": [
        "### Declare the values that we are going to optimize\n",
        "\n",
        "class Parameters(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Parameters, self).__init__()\n",
        "    self.data = .5*torch.randn(batch_size, 256, size1//16, size2//16).cuda() # 1x256x14x15 (225/16, 400/16)\n",
        "    self.data = torch.nn.Parameter(torch.sin(self.data))\n",
        "\n",
        "  def forward(self):\n",
        "    return self.data\n",
        "\n",
        "def init_params():\n",
        "  params=Parameters().cuda()\n",
        "  optimizer = torch.optim.AdamW([{'params':[params.data], 'lr': learning_rate}], weight_decay=wd)\n",
        "  return params, optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ij3A0d-xzNwX"
      },
      "outputs": [],
      "source": [
        "def MusicMood(song_path):\n",
        "  # Load the audio file\n",
        "  audio, sr = librosa.load(song_path)\n",
        "\n",
        "  # Extract audio features\n",
        "  tempo, beats = librosa.beat.beat_track(y=audio, sr=sr)\n",
        "  chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
        "  mfcc = librosa.feature.mfcc(y=audio, sr=sr)\n",
        "\n",
        "  # Analyze the features and extract the music mood\n",
        "  mood = analyzeMusicMood(tempo, beats, chroma, mfcc)\n",
        "\n",
        "  return mood\n",
        "\n",
        "def analyzeMusicMood(tempo, beats, chroma, mfcc):\n",
        "    # Ensure that tempo is a NumPy array\n",
        "    if not isinstance(tempo, np.ndarray):\n",
        "        tempo = np.array([tempo])\n",
        "\n",
        "    # Calculate the average tempo and beat variance\n",
        "    avg_tempo = tempo.mean()\n",
        "    beat_var = beats.var()\n",
        "\n",
        "    # Calculate the chroma and MFCC mean values\n",
        "    chroma_mean = chroma.mean(axis=1)\n",
        "    mfcc_mean = mfcc.mean(axis=1)\n",
        "\n",
        "    # Based on the calculated features, determine the music mood\n",
        "    if avg_tempo > 120 and beat_var < 100 and chroma_mean[3] > 0.5:\n",
        "        mood = 'Happy'  # Happy\n",
        "    elif avg_tempo < 100 and chroma_mean[1] > 0.3 and mfcc_mean[5] > -5:\n",
        "        mood = 'Sad'  # Sad\n",
        "    else:\n",
        "        mood = 'Neutral'  # Neutral\n",
        "\n",
        "    return mood\n",
        "\n",
        "\n",
        "### Encoding prompts and a few more things\n",
        "normalize = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "def encodeText(text):\n",
        "  t=clip.tokenize(text).cuda()\n",
        "  t=clipmodel.encode_text(t).detach().clone()\n",
        "  return t\n",
        "\n",
        "def createEncodings(include, exclude, extras):\n",
        "  include_enc=[]\n",
        "  for text in include:\n",
        "    include_enc.append(encodeText(text))\n",
        "  exclude_enc=encodeText(exclude) if exclude != '' else 0\n",
        "  extras_enc=encodeText(extras) if extras !='' else 0\n",
        "\n",
        "  return include_enc, exclude_enc, extras_enc\n",
        "\n",
        "augTransform = torch.nn.Sequential(\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.RandomAffine(30, (.2, .2), fill=0)\n",
        ").cuda()\n",
        "\n",
        "Params, optimizer = init_params()\n",
        "\n",
        "with torch.no_grad():\n",
        "  print(Params().shape)\n",
        "  img= norm_data(generator(Params()).cpu()) # 1 x 3 x 224 x 400 [225 x 400]\n",
        "  print(\"img dimensions: \",img.shape)\n",
        "  show_from_tensor(img[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mVTkMFhxzPc0"
      },
      "outputs": [],
      "source": [
        "### create crops\n",
        "\n",
        "def create_crops(img, num_crops=32):\n",
        "  p=size1//2\n",
        "  img = torch.nn.functional.pad(img, (p,p,p,p), mode='constant', value=0) # 1 x 3 x 448 x 624 (adding 112*2 on all sides to 224x400)\n",
        "\n",
        "  img = augTransform(img) #RandomHorizontalFlip and RandomAffine\n",
        "\n",
        "  crop_set = []\n",
        "  for ch in range(num_crops):\n",
        "    gap1= int(torch.normal(1.2, .3, ()).clip(.43, 1.9) * size1)\n",
        "    offsetx = torch.randint(0, int(size1*2-gap1),())\n",
        "    offsety = torch.randint(0, int(size1*2-gap1),())\n",
        "\n",
        "    crop=img[:,:,offsetx:offsetx+gap1, offsety:offsety+gap1]\n",
        "\n",
        "    crop = torch.nn.functional.interpolate(crop,(224,224), mode='bilinear', align_corners=True)\n",
        "    crop_set.append(crop)\n",
        "\n",
        "  img_crops=torch.cat(crop_set,0) ## 30 x 3 x 224 x 224\n",
        "\n",
        "  randnormal = torch.randn_like(img_crops, requires_grad=False)\n",
        "  num_rands=0\n",
        "  randstotal=torch.rand((img_crops.shape[0],1,1,1)).cuda() #32\n",
        "\n",
        "  for ns in range(num_rands):\n",
        "    randstotal*=torch.rand((img_crops.shape[0],1,1,1)).cuda()\n",
        "\n",
        "  img_crops = img_crops + noise_factor*randstotal*randnormal\n",
        "\n",
        "  return img_crops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8l7daMSXzR5o"
      },
      "outputs": [],
      "source": [
        "### Show current state of generation\n",
        "\n",
        "def showme(Params, show_crop):\n",
        "  with torch.no_grad():\n",
        "    generated = generator(Params())\n",
        "\n",
        "    if (show_crop):\n",
        "      aug_gen = generated.float() # 1 x 3 x 224 x 400\n",
        "      aug_gen = create_crops(aug_gen, num_crops=1)\n",
        "      aug_gen_norm = norm_data(aug_gen[0])\n",
        "      #show_from_tensor(aug_gen_norm)\n",
        "\n",
        "    latest_gen=norm_data(generated.cpu()) # 1 x 3 x 224 x 400\n",
        "    show_from_tensor(latest_gen[0])\n",
        "\n",
        "  return (latest_gen[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8D5xZDjlzUMN"
      },
      "outputs": [],
      "source": [
        "# Optimization process\n",
        "\n",
        "def optimize_result(Params, prompt):\n",
        "  alpha=1 ## the importance of the include encodings\n",
        "  beta=.5 ## the importance of the exclude encodings\n",
        "\n",
        "  ## image encoding\n",
        "  out = generator(Params())\n",
        "  out = norm_data(out)\n",
        "  out = create_crops(out)\n",
        "  out = normalize(out) # 30 x 3 x 224 x 224\n",
        "  image_enc=clipmodel.encode_image(out) ## 30 x 512\n",
        "\n",
        "  ## text encoding  w1 and w2\n",
        "  final_enc = w1*prompt + w1*extras_enc # prompt and extras_enc : 1 x 512\n",
        "  final_text_include_enc = final_enc / final_enc.norm(dim=-1, keepdim=True) # 1 x 512\n",
        "  final_text_exclude_enc = exclude_enc\n",
        "\n",
        "  ## calculate the loss\n",
        "  main_loss = torch.cosine_similarity(final_text_include_enc, image_enc, -1) # 30\n",
        "  penalize_loss = torch.cosine_similarity(final_text_exclude_enc, image_enc, -1) # 30\n",
        "\n",
        "  final_loss = -alpha*main_loss + beta*penalize_loss\n",
        "\n",
        "  return final_loss\n",
        "\n",
        "def optimize(Params, optimizer, prompt):\n",
        "  loss = optimize_result(Params, prompt).mean()\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "eruRhtNJzVry"
      },
      "outputs": [],
      "source": [
        "# Create a tqdm progress bar for the entire training process\n",
        "def training_loop(Params, optimizer, show_crop=False):\n",
        "    res_img = []\n",
        "    res_z = []\n",
        "\n",
        "    total_iterations = len(include_enc) * total_iter\n",
        "    current_iteration = 0\n",
        "\n",
        "    # Create a tqdm progress bar for the entire training process\n",
        "    total_progress_bar = tqdm(total=total_iterations, desc=\"Generating\")\n",
        "\n",
        "    for prompt in include_enc:\n",
        "        iteration = 0\n",
        "        Params, optimizer = init_params()  # Initialize Params and optimizer for each prompt\n",
        "\n",
        "        for it in range(total_iter):\n",
        "            loss = optimize(Params, optimizer, prompt)\n",
        "\n",
        "            if iteration >= 80 and iteration % show_step == 0:\n",
        "                new_img = showme(Params, show_crop)\n",
        "                res_img.append(new_img)\n",
        "                res_z.append(Params())  # 1 x 256 x 14 x 25\n",
        "                print(\"loss:\", loss.item(), \"\\niteration:\", iteration)\n",
        "\n",
        "            current_iteration += 1\n",
        "            total_progress_bar.update(1)  # Update the total progress bar\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "    # Close the total progress bar\n",
        "    total_progress_bar.close()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    return res_img, res_z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbnDAET_FL9L"
      },
      "outputs": [],
      "source": [
        "print(\"=======================================\")\n",
        "print(\"|  Welcome to the Song Mood Visualizer |\")\n",
        "print(\"| This program generates visuals based |\")\n",
        "print(\"|   on the mood of a song.             |\")\n",
        "print(\"=======================================\")\n",
        "\n",
        "# Function to generate images\n",
        "def generate_images(event):\n",
        "    clear_output()\n",
        "\n",
        "    # Get the YouTube link from the text input widget\n",
        "    youtube_link = song_link_input.value\n",
        "\n",
        "    # Create a YouTube object and get the best audio stream\n",
        "    yt = pytube.YouTube(youtube_link)\n",
        "    audio_stream = yt.streams.filter(only_audio=True).first()\n",
        "\n",
        "    # Get the song's title and artist name from YouTube\n",
        "    song_title = yt.title\n",
        "    artist_name = yt.author\n",
        "\n",
        "    # Download the audio stream to a file\n",
        "    audio_stream.download(output_path=\"/content\", filename=f\"{song_title}.mp3\")\n",
        "\n",
        "    # Set the path to the downloaded MP3 file\n",
        "    song_path = f\"/content/{song_title}.mp3\"\n",
        "\n",
        "    # Display song information\n",
        "    print(\"=======================================\")\n",
        "    print(f\"  Song Information                   \")\n",
        "    print(f\"  Title: {song_title}                \")\n",
        "    print(f\"  Artist: {artist_name}              \")\n",
        "    print(\"=======================================\")\n",
        "\n",
        "    # Get the cover image of the song from YouTube and display it\n",
        "    response = requests.get(yt.thumbnail_url)\n",
        "    img = PILImage.open(BytesIO(response.content))\n",
        "    display(img)\n",
        "\n",
        "    include = []\n",
        "    # Determine the mood of the song\n",
        "    song_mood = MusicMood(song_path)\n",
        "    print(\"\\n\\n=======================================\")\n",
        "    print(f\"  Mood: {song_mood}                      \")\n",
        "    print(\"=======================================\")\n",
        "\n",
        "    if song_mood == 'Happy':  # Happy mood\n",
        "            include.extend([\n",
        "                f'Create a joyful and lively scene that matches the happy mood of the song \"{song_title}\"',\n",
        "                f'song \"{song_title}\"',\n",
        "                f'Generate a vibrant and colorful representation of happiness in the song \"{song_title}\".'\n",
        "            ])\n",
        "    elif song_mood == 'Sad':  # Sad mood\n",
        "        include.extend([\n",
        "            f'Capture the melancholic and somber atmosphere that resonates with the sad mood of the song \"{song_title}\" ',\n",
        "            f'song \"{song_title}',\n",
        "            f'Create a quiet and reflective scene that captures the sadness in the song \"{song_title}\".'\n",
        "        ])\n",
        "    elif song_mood == 'Neutral':  # Neutral mood\n",
        "        include.extend([\n",
        "            f'Craft a calm and peaceful landscape that reflects the neutral mood of the song \"{song_title}\"',\n",
        "            f'song \"{song_title}\"',\n",
        "            f'Generate a serene and tranquil environment that mirrors the song\\'s neutral emotions in \"{song_title}\".'\n",
        "    ])\n",
        "\n",
        "    global include_enc, exclude_enc, extras_enc, w1, w2, noise_factor, total_iter, show_step, res_z, res_img\n",
        "\n",
        "    # Define include_enc, exclude_enc, and extras_enc based on prompts\n",
        "    exclude = 'watermark, cropped, confusing, incoherent, cut, blurry'\n",
        "    extras = 'concise, detailed, high-quality, creative, realistic, surreal, historical, futuristic, mystical, educational, humorous'\n",
        "    w1 = 1\n",
        "    w2 = 1\n",
        "    noise_factor = 0.22\n",
        "    total_iter = 110\n",
        "    show_step = 20  # set this to see the result every 20 iterations beyond iteration 80\n",
        "\n",
        "    include_enc, exclude_enc, extras_enc = createEncodings(include, exclude, extras)\n",
        "\n",
        "    # Now you can call training_loop\n",
        "    res_img, res_z = training_loop(Params, optimizer, show_crop=True)\n",
        "\n",
        "    # Display images or results here\n",
        "    display(\"Images generated successfully!\")  # You can replace this with displaying images\n",
        "\n",
        "# Create a text input widget for the YouTube link\n",
        "song_link_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter YouTube link',\n",
        "    description='YouTube Link:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "# Create a button widget for generating images\n",
        "generate_button = widgets.Button(\n",
        "    description='Generate Images',\n",
        "    disabled=False,\n",
        "    button_style='',  # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Click to generate images',\n",
        "    icon='check'  # (FontAwesome names without the `fa-` prefix)\n",
        ")\n",
        "\n",
        "# Attach the click handler to the button\n",
        "generate_button.on_click(generate_images)\n",
        "\n",
        "# Display widgets\n",
        "display(song_link_input, generate_button)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9_Di2Js7zqum"
      },
      "outputs": [],
      "source": [
        "def interpolate(res_z_list, duration_list):\n",
        "  gen_img_list=[]\n",
        "  fps = 25\n",
        "\n",
        "  for idx, (z, duration) in enumerate(zip(res_z_list, duration_list)):\n",
        "    num_steps = int(duration*fps)\n",
        "    z1=z\n",
        "    z2=res_z_list[(idx+1)%len(res_z_list)] # 1 x 256 x 14 x 25 (225/16, 400/16)\n",
        "\n",
        "    for step in range(num_steps):\n",
        "      alpha = math.sin(1.5*step/num_steps)**6\n",
        "      z_new = alpha * z2 + (1-alpha) * z1\n",
        "\n",
        "      new_gen=norm_data(generator(z_new).cpu())[0] ## 3 x 224 x 400\n",
        "      new_img=T.ToPILImage(mode='RGB')(new_gen)\n",
        "      gen_img_list.append(new_img)\n",
        "\n",
        "  return gen_img_list\n",
        "\n",
        "durations=[5,5,5,5,5,5]\n",
        "interp_result_img_list = interpolate(res_z, durations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "cQ-u4Bh4zu5A"
      },
      "outputs": [],
      "source": [
        "## create a video\n",
        "out_video_path=f\"../video.mp4\"\n",
        "writer = imageio.get_writer(out_video_path, fps=25)\n",
        "for pil_img in interp_result_img_list:\n",
        "  img = np.array(pil_img, dtype=np.uint8)\n",
        "  writer.append_data(img)\n",
        "\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMj8tAhmzwcn"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open('../video.mp4','rb').read()\n",
        "data=\"data:video/mp4;base64,\"+b64encode(mp4).decode()\n",
        "HTML(\"\"\"<video width=800 controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO4ppPyZTUaGE3TjKhmsvwc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}